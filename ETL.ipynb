{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b470ea8a-aba0-4063-982b-82546ca52341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "from pyspark.sql.functions import col, when, avg, round\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f2f68c-8073-48e4-a277-26334d13dcb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --- Step 1: Extract ---\n",
    "def fetch_data(url, headers=None, retries=3, backoff=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            import time; time.sleep(backoff ** (attempt+1))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc48beb-2470-48fc-a52e-5a3d49515fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "region = \"US-CA\"\n",
    "url = f\"https://api.ebird.org/v2/data/obs/{region}/recent\"\n",
    "\n",
    "# You may need an API key in header\n",
    "headers = {\"X-eBirdApiToken\": \"x\"}\n",
    "\n",
    "data = fetch_data(url, headers=headers)\n",
    "if not data:\n",
    "    raise Exception(\"Failed to fetch data from eBird API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "345c4b33-139f-4027-b0b1-76f4f47695a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['speciesCode', 'comName', 'sciName', 'locId', 'locName', 'obsDt',\n       'howMany', 'lat', 'lng', 'obsValid', 'obsReviewed', 'locationPrivate',\n       'subId', 'exoticCategory'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.json_normalize(data)\n",
    "# Inspect what columns are present\n",
    "print(pdf.columns)\n",
    "\n",
    "df_obs = pdf[[\n",
    "    'speciesCode',\n",
    "    'comName',\n",
    "    'sciName',\n",
    "    'obsDt',\n",
    "    'howMany',\n",
    "    'lat',\n",
    "    'lng',\n",
    "    'locName'\n",
    "]].drop_duplicates()\n",
    "\n",
    "# Convert to Spark\n",
    "spark_df = spark.createDataFrame(df_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d940b9-1428-49d8-a1de-1cee1545d5c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"observation_date\", col(\"obsDt\")) \\\n",
    "                   .withColumn(\"count\", col(\"howMany\").cast(\"integer\")) \\\n",
    "                   .withColumn(\"latitude\", col(\"lat\").cast(\"double\")) \\\n",
    "                   .withColumn(\"longitude\", col(\"lng\").cast(\"double\")) \\\n",
    "                   .withColumn(\"common_name\", col(\"comName\")) \\\n",
    "                   .withColumn(\"scientific_name\", col(\"sciName\")) \\\n",
    "                   .withColumn(\"location_name\", col(\"locName\")) \\\n",
    "                   .select(\n",
    "                        \"speciesCode\",\n",
    "                        \"common_name\",\n",
    "                        \"scientific_name\",\n",
    "                        \"observation_date\",\n",
    "                        \"count\",\n",
    "                        \"latitude\",\n",
    "                        \"longitude\",\n",
    "                        \"location_name\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63de297f-ffc2-4747-99ad-23ee57c62641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL for eBird API complete.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS ebird_db\n",
    "COMMENT 'Database for eBird observations'\n",
    "\"\"\")\n",
    "\n",
    "# Write table\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ebird_db.ebird_recent_observations\")\n",
    "\n",
    "print(\"ETL for eBird API complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473787ca-8938-49ab-a0fb-77a8493e8856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count as f_count, avg, to_date\n",
    "\n",
    "# Load raw observations\n",
    "df = spark.table(\"ebird_db.ebird_recent_observations\")\n",
    "\n",
    "# Ensure proper types\n",
    "df = df.withColumn(\"observation_date\", to_date(col(\"observation_date\")))\n",
    "\n",
    "# --- Aggregation 1: Most common species observed ---\n",
    "species_count = (\n",
    "    df.groupBy(\"common_name\", \"scientific_name\")\n",
    "      .agg(f_count(\"*\").alias(\"num_observations\"),\n",
    "           avg(\"count\").alias(\"avg_count\"))\n",
    "      .orderBy(col(\"num_observations\").desc())\n",
    ")\n",
    "\n",
    "species_count.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"ebird_db.analytics_species_count\")\n",
    "\n",
    "# --- Aggregation 2: Observations by date ---\n",
    "observations_by_date = (\n",
    "    df.groupBy(\"observation_date\")\n",
    "      .agg(f_count(\"*\").alias(\"num_observations\"))\n",
    "      .orderBy(\"observation_date\")\n",
    ")\n",
    "\n",
    "observations_by_date.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"ebird_db.analytics_observations_by_date\")\n",
    "\n",
    "# --- Aggregation 3: Observations by location ---\n",
    "observations_by_location = (\n",
    "    df.groupBy(\"location_name\")\n",
    "      .agg(f_count(\"*\").alias(\"num_observations\"))\n",
    "      .orderBy(col(\"num_observations\").desc())\n",
    ")\n",
    "\n",
    "observations_by_location.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"ebird_db.analytics_observations_by_location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3135bc35-e3f2-4e36-8a97-441654eb0189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0ca151-78bf-4419-847e-760bdddd4689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd5bbf4-65e1-436a-a1d5-51eef5743117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5816231204078125,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}